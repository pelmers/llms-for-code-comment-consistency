[
  {
    "html_url": "https://github.com/falconry/falcon/pull/706#discussion_r53693673",
    "path": "falcon/errors.py",
    "line": 379.0,
    "body": "At first, I was about to remove it but it generated some errors in tests so I had to put it again an I forgot to add the related docstring. Let me fix that.\n",
    "user": "davidbgk",
    "diff_hunk": "@@ -361,6 +361,25 @@ def __init__(self, title, description, retry_after=None, **kwargs):\n                                                   **kwargs)\n \n \n+class HTTPUnavailableForLegalReasons(OptionalRepresentation, HTTPError):\n+    \"\"\"451 Unavailable For Legal Reasons.\n+\n+    This status code indicates that the server is denying access to the\n+    resource as a consequence of a legal demand.\n+\n+    See also:\n+    https://datatracker.ietf.org/doc/draft-ietf-httpbis-legally-restricted-status/\n+\n+    Args:\n+        kwargs (optional): Same as for ``HTTPError``.\n+\n+    \"\"\"\n+\n+    def __init__(self, title, **kwargs):",
    "author_association": "CONTRIBUTOR",
    "commit_id": "8c6acc295ad8e5969ffdcbb93737e3dcd395d7c2",
    "id": 53693673,
    "repo": "falconry/falcon",
    "extension": ".py"
  },
  {
    "html_url": "https://github.com/DanielNoord/pydocstringformatter/pull/41#discussion_r803446208",
    "path": "pydocstringformatter/formatting/formatter.py",
    "line": 109.0,
    "body": "Hmm, good question.\r\n\r\nLet's wait for a user to complain about this. Honestly, why would you want to strip whitespaces from end of lines but not from the start of your docstring? The only reason I can think of is if we're not stripping correctly, which is something we should fix if we get any reports about it \ud83d\ude04 ",
    "user": "DanielNoord",
    "diff_hunk": "@@ -103,3 +103,24 @@ def _treat_string(self, tokeninfo: tokenize.TokenInfo, indent_length: int) -> st\n                         + tokeninfo.string[index + 2 :]\n                     )\n         return tokeninfo.string\n+\n+\n+class StripWhitespacesFormatter(StringFormatter):\n+    \"\"\"Strip 1) docstring start, 2) docstring end and 3) end of line.\"\"\"",
    "author_association": "OWNER",
    "commit_id": "893742a766dfcf50b15084ce104b0ffef752fe7d",
    "id": 803446208,
    "repo": "DanielNoord/pydocstringformatter",
    "extension": ".py"
  },
  {
    "html_url": "https://github.com/BlueBrain/NeuroM/pull/755#discussion_r597166575",
    "path": "neurom/core/_soma.py",
    "line": 153.0,
    "body": "Its docstring uses `points`. This needs to be updated to `morphio_soma`",
    "user": "asanin-epfl",
    "diff_hunk": "@@ -131,7 +137,7 @@ def volume(self):\n     def __str__(self):\n         \"\"\"Return a string representation.\"\"\"\n         return ('SomaCylinders(%s) <center: %s, virtual radius: %s>' %\n-                (repr(self._points), self.center, self.radius))\n+                (repr(self.points), self.center, self.radius))\n \n \n class SomaNeuromorphoThreePointCylinders(SomaCylinders):",
    "author_association": "CONTRIBUTOR",
    "commit_id": "eab1c99cef23fbdb6314f444d180f169af5f58cb",
    "id": 597166575,
    "repo": "BlueBrain/NeuroM",
    "extension": ".py"
  },
  {
    "html_url": "https://github.com/sympy/sympy/pull/1181#discussion_r614000",
    "path": "sympy/functions/combinatorial/numbers.py",
    "line": 394.0,
    "body": "No, it is not.\nMust be fixed to keep the correlation of functions' interface as it is described in the docstring.\n",
    "user": "goodok",
    "diff_hunk": "@@ -359,15 +361,48 @@ def _bell_poly(n, prev):\n         for k in xrange(2, n+1):\n             a = a * (n-k+1) // (k-1)\n             s += a * prev[k-1]\n-        return (_sym * s).expand()\n+        return expand_mul(_sym * s)\n+\n+    @staticmethod\n+    #@assoc_recurrence_memo([[S.One]])\n+    def _bell_incomplete_poly(n, k, symbols):\n+        r\"\"\"\n+        The second kind of Bell polynomials (incomplete Bell polynomials).\n+\n+        Calculated by recurrence formula:\n+\n+        .. math:: B_{n,k}(x_1, x_2, \\dotsc, x_{n-k+1}) =\n+                \\sum_{m=1}^{n-k+1}\n+                \\x_m \\binom{n-1}{m-1} B_{n-m,k-1}(x_1, x_2, \\dotsc, x_{n-m-k})\n+\n+        where\n+            B_{0,0} = 1;\n+            B_{n,0} = 0; for n>=1\n+            B_{0,k} = 0; for k>=1\n+\n+        \"\"\"\n+        if (n==0) and (k==0):\n+            return S.One\n+        elif (n==0) or (k==0):\n+            return S.Zero\n+        #s = Poly(S.Zero, symbols)\n+        s = S.Zero\n+        a = S.One\n+        for m in xrange(1, n-k+2):\n+            s += a*bell._bell_incomplete_poly(n-m, k-1, symbols)*symbols[m]\n+            a = a*(n-m)/m\n+        return expand_mul(s)",
    "author_association": "CONTRIBUTOR",
    "commit_id": "7ed71617b7c54b8b6201d9ca2012b4c512aa5470",
    "id": 614000,
    "repo": "sympy/sympy",
    "extension": ".py"
  },
  {
    "html_url": "https://github.com/django-json-api/django-rest-framework-json-api/pull/1101#discussion_r1018709309",
    "path": "rest_framework_json_api/views.py",
    "line": 132.0,
    "body": "I only reformatted that comment into a proper docstring to satisfy bugbear. The comment is really not that clear and helpful, though. Added it to my list TODO to change it (most likely with a general docstring update).",
    "user": "sliverc",
    "diff_hunk": "@@ -129,7 +129,10 @@ def get_queryset(self, *args, **kwargs):\n \n class RelatedMixin:\n     \"\"\"\n-    This mixin handles all related entities, whose Serializers are declared in \"related_serializers\"\n+    Mixing handling related links.",
    "author_association": "MEMBER",
    "commit_id": "62f2a7d87690bbcf04c2c3f41dc394853f539977",
    "id": 1018709309,
    "repo": "django-json-api/django-rest-framework-json-api",
    "extension": ".py"
  },
  {
    "html_url": "https://github.com/dask/dask/pull/1927#discussion_r97657046",
    "path": "dask/base.py",
    "line": 476.0,
    "body": "I chose not to change this.  Now rather than copy-paste-modifying the docstring for persist I explicitly copy it (in Python 3 where this is allowed).  The ambiguity of the old language overlaps more nicely in the method case.  There is a trade-off here obviously, but I felt that copying the docstring was worth avoiding.",
    "user": "mrocklin",
    "diff_hunk": "@@ -383,3 +387,151 @@ def tokenize(*args, **kwargs):\n     if kwargs:\n         args = args + (kwargs,)\n     return md5(str(tuple(map(normalize_token, args))).encode()).hexdigest()\n+\n+\n+def collections_to_dsk(collections, optimize_graph=True, **kwargs):\n+    \"\"\"\n+    Convert many collections into a single dask graph, after optimization\n+    \"\"\"\n+    optimizations = (kwargs.pop('optimizations', None) or\n+                     _globals.get('optimizations', []))\n+    if optimize_graph:\n+        groups = groupby(lambda x: x._optimize, collections)\n+        groups = {opt: _extract_graph_and_keys(val)\n+                  for opt, val in groups.items()}\n+        for opt in optimizations:\n+            groups = {k: [opt(dsk, keys), keys]\n+                      for k, (dsk, keys) in groups.items()}\n+        dsk = merge([opt(dsk, keys, **kwargs)\n+                     for opt, (dsk, keys) in groups.items()])\n+    else:\n+        dsk = merge(c.dask for c in collections)\n+\n+    return dsk\n+\n+\n+def _extract_graph_and_keys(vals):\n+    \"\"\"Given a list of dask vals, return a single graph and a list of keys such\n+    that ``get(dsk, keys)`` is equivalent to ``[v.compute() v in vals]``.\"\"\"\n+    dsk = {}\n+    keys = []\n+    for v in vals:\n+        # Optimization to avoid merging dictionaries in Delayed values. Reduces\n+        # memory usage for large graphs.\n+        if hasattr(v, '_dasks'):\n+            for d in v._dasks:\n+                dsk.update(d)\n+        else:\n+            dsk.update(v.dask)\n+        keys.append(v._keys())\n+\n+    return dsk, keys\n+\n+\n+def redict_collection(c, dsk):\n+    from dask.delayed import Delayed\n+    if isinstance(c, Delayed):\n+        return Delayed(c.key, [dsk])\n+    else:\n+        cc = copy.copy(c)\n+        cc.dask = dsk\n+        return cc\n+\n+\n+def persist(*args, **kwargs):\n+    \"\"\" Persist multiple Dask collections into memory\n+\n+    This turns lazy Dask collections into Dask collections with the same\n+    metadata, but now with their results actively computing.\n+\n+    For example a lazy dask.array built up from many lazy calls will now be a\n+    dask.array of the same shape, dtype, etc., but now with all of those lazy\n+    operations either in memory (for single-machine computing) or actively\n+    computing asynchronously (for distributed computing).\n+\n+    If using Dask on a single machine then you should ensure that the\n+    dataset fits entirely within memory.\n+\n+    This function operates differently if a ``dask.distributed.Client`` exists\n+    and is connected to a distributed scheduler.  In this case this function\n+    will return as soon as the task graph has been submitted to the cluster,\n+    but before the computations have completed.  Computations will continue\n+    asynchronously in the background.  When using this function with the single\n+    machine scheduler it blocks until the computations have finished.\n+\n+    Examples\n+    --------\n+\n+    >>> df = dd.read_csv('/path/to/*.csv')  # doctest: +SKIP\n+    >>> df = df[df.name == 'Alice']  # doctest: +SKIP\n+    >>> df['in-debt'] = df.balance < 0  # doctest: +SKIP\n+    >>> df = df.persist()  # triggers computation  # doctest: +SKIP\n+\n+    >>> df.value().min()  # future computations are now fast  # doctest: +SKIP\n+    -10\n+    >>> df.value().max()  # doctest: +SKIP\n+    100\n+\n+    >>> from dask import persist  # use persist function on multiple collections\n+    >>> a, b = persist(a, b)  # doctest: +SKIP\n+\n+    Parameters\n+    ----------\n+    args: Dask collections\n+    get : callable, optional\n+        A scheduler ``get`` function to use. If not provided, the default\n+        is to check the global settings first, and then fall back to\n+        the collection defaults.\n+    optimize_graph : bool, optional\n+        If True [default], the graph is optimized before computation.\n+        Otherwise the graph is run as is. This can be useful for debugging.\n+    kwargs\n+        Extra keywords to forward to the scheduler ``get`` function.\n+\n+    Returns\n+    -------\n+    New dask collections backed by in-memory data",
    "author_association": "MEMBER",
    "commit_id": "8b8ff8ad791677bdc9db30e7f337636df240114f",
    "id": 97657046,
    "repo": "dask/dask",
    "extension": ".py"
  },
  {
    "html_url": "https://github.com/NREL/reV/pull/350#discussion_r817010668",
    "path": "reV/hybrids/hybrids.py",
    "line": 340.0,
    "body": "These are great docstrings. Very descriptive and helps the user understand how they should interact with the class. For example, the note about the prefixes is great. The user would never guess this (but obviously it makes a lot of sense when you think about it)",
    "user": "grantbuster",
    "diff_hunk": "@@ -0,0 +1,1170 @@\n+# -*- coding: utf-8 -*-\n+\"\"\"reV Hybridization module.\n+\n+@author: ppinchuk\n+\"\"\"\n+\n+import logging\n+import numpy as np\n+import re\n+import pandas as pd\n+from string import ascii_letters\n+from warnings import warn\n+from collections import namedtuple\n+\n+from reV.handlers.outputs import Outputs\n+from reV.utilities.exceptions import (FileInputError, InputError,\n+                                      InputWarning, OutputWarning)\n+from reV.hybrids.hybrid_methods import HYBRID_METHODS\n+\n+from rex.resource import Resource\n+from rex.utilities.utilities import to_records_array\n+\n+logger = logging.getLogger(__name__)\n+\n+MERGE_COLUMN = 'sc_point_gid'\n+PROFILE_DSET_REGEX = 'rep_profiles_[0-9]+$'\n+SOLAR_PREFIX = 'solar_'\n+WIND_PREFIX = 'wind_'\n+NON_DUPLICATE_COLS = {\n+    'latitude', 'longitude', 'country', 'state', 'county', 'elevation',\n+    'timezone', 'sc_point_gid', 'sc_row_ind', 'sc_col_ind'\n+}\n+DROPPED_COLUMNS = ['gid']\n+DEFAULT_FILL_VALUES = {'solar_capacity': 0, 'wind_capacity': 0,\n+                       'solar_mean_cf': 0, 'wind_mean_cf': 0}\n+OUTPUT_PROFILE_NAMES = ['hybrid_profile',\n+                        'hybrid_solar_profile',\n+                        'hybrid_wind_profile']\n+RatioColumns = namedtuple('RatioColumns', ['num', 'denom', 'fixed'],\n+                          defaults=(None, None, None))\n+\n+\n+class ColNameFormatter:\n+    \"\"\"Column name formatting helper class. \"\"\"\n+    ALLOWED = set(ascii_letters)\n+\n+    @classmethod\n+    def fmt(cls, n):\n+        \"\"\"Format an input column name to remove excess chars and whitespace.\n+\n+        This method should help facilitate the merging of column names\n+        between two DataFrames.\n+\n+        Parameters\n+        ----------\n+        n : str\n+            Input column name.\n+\n+        Returns\n+        -------\n+        str\n+            The column name with all characters except ascii stripped\n+            and all lowercase.\n+        \"\"\"\n+        return ''.join(c for c in n if c in cls.ALLOWED).lower()\n+\n+\n+class HybridsData:\n+    \"\"\"Hybrids input data container. \"\"\"\n+\n+    def __init__(self, solar_fpath, wind_fpath):\n+        \"\"\"\n+        Parameters\n+        ----------\n+        solar_fpath : str\n+            Filepath to rep profile output file to extract solar profiles and\n+            summaries from.\n+        wind_fpath : str\n+            Filepath to rep profile output file to extract wind profiles and\n+            summaries from.\n+        \"\"\"\n+        self.solar_fpath = solar_fpath\n+        self.wind_fpath = wind_fpath\n+        self.profile_dset_names = []\n+        self.merge_col_overlap_values = set()\n+        self._solar_meta = None\n+        self._wind_meta = None\n+        self._solar_time_index = None\n+        self._wind_time_index = None\n+        self._hybrid_time_index = None\n+        self.__profile_reg_check = re.compile(PROFILE_DSET_REGEX)\n+        self.__solar_cols = self.solar_meta.columns.map(ColNameFormatter.fmt)\n+        self.__wind_cols = self.wind_meta.columns.map(ColNameFormatter.fmt)\n+\n+    @property\n+    def solar_meta(self):\n+        \"\"\"Summary for the solar representative profiles.\n+\n+        Returns\n+        -------\n+        solar_meta : pd.DataFrame\n+            Summary for the solar representative profiles.\n+        \"\"\"\n+        if self._solar_meta is None:\n+            with Resource(self.solar_fpath) as res:\n+                self._solar_meta = res.meta\n+        return self._solar_meta\n+\n+    @property\n+    def wind_meta(self):\n+        \"\"\"Summary for the wind representative profiles.\n+\n+        Returns\n+        -------\n+        wind_meta : pd.DataFrame\n+            Summary for the wind representative profiles.\n+        \"\"\"\n+        if self._wind_meta is None:\n+            with Resource(self.wind_fpath) as res:\n+                self._wind_meta = res.meta\n+        return self._wind_meta\n+\n+    @property\n+    def solar_time_index(self):\n+        \"\"\"Get the time index for the solar rep profiles.\n+\n+        Returns\n+        -------\n+        solar_time_index : pd.datetimeindex\n+            Time index sourced from the solar reV gen file.\n+        \"\"\"\n+        if self._solar_time_index is None:\n+            with Resource(self.solar_fpath) as res:\n+                self._solar_time_index = res.time_index\n+        return self._solar_time_index\n+\n+    @property\n+    def wind_time_index(self):\n+        \"\"\"Get the time index for the wind rep profiles.\n+\n+        Returns\n+        -------\n+        wind_time_index : pd.datetimeindex\n+            Time index sourced from the wind reV gen file.\n+        \"\"\"\n+        if self._wind_time_index is None:\n+            with Resource(self.wind_fpath) as res:\n+                self._wind_time_index = res.time_index\n+        return self._wind_time_index\n+\n+    @property\n+    def hybrid_time_index(self):\n+        \"\"\"Get the time index for the hybrid rep profiles.\n+\n+        Returns\n+        -------\n+        hybrid_time_index : pd.datetimeindex\n+            Time index for the hybrid rep profiles.\n+        \"\"\"\n+        if self._hybrid_time_index is None:\n+            self._hybrid_time_index = self.solar_time_index.join(\n+                self.wind_time_index, how='inner')\n+        return self._hybrid_time_index\n+\n+    def contains_col(self, col_name):\n+        \"\"\"Check if input column name exists in either meta data set.\n+\n+        Parameters\n+        ----------\n+        col_name : str\n+            Name of column to check for.\n+\n+        Returns\n+        -------\n+        bool\n+            Whether or not the column is found in either meta data set.\n+        \"\"\"\n+        fmt_name = ColNameFormatter.fmt(col_name)\n+        col_in_solar = fmt_name in self.__solar_cols\n+        col_in_wind = fmt_name in self.__wind_cols\n+        return col_in_solar or col_in_wind\n+\n+    def validate(self):\n+        \"\"\"Validate the input data.\n+\n+        This method checks for a minimum time index length, a unique\n+        profile, and unique merge column that overlaps between both data\n+        sets.\n+\n+        \"\"\"\n+        self._validate_time_index()\n+        self._validate_num_profiles()\n+        self._validate_merge_col_exists()\n+        self._validate_unique_merge_col()\n+        self._validate_merge_col_overlaps()\n+\n+    def _validate_time_index(self):\n+        \"\"\"Validate the hybrid time index to be of len >= 8760.\n+\n+        Raises\n+        ------\n+        FileInputError\n+            If len(time_index) < 8760 for the hybrid profile.\n+        \"\"\"\n+        if len(self.hybrid_time_index) < 8760:\n+            msg = (\"The length of the merged time index ({}) is less than \"\n+                   \"8760. Please ensure that the input profiles have a \"\n+                   \"time index that overlaps >= 8760 times.\")\n+            e = msg.format(len(self.hybrid_time_index))\n+            logger.error(e)\n+            raise FileInputError(e)\n+\n+    def _validate_num_profiles(self):\n+        \"\"\"Validate the number of input profiles.\n+\n+        Raises\n+        ------\n+        FileInputError\n+            If # of rep_profiles > 1.\n+        \"\"\"\n+        for fp in [self.solar_fpath, self.wind_fpath]:\n+            with Resource(fp) as res:\n+                profile_dset_names = [\n+                    n for n in res.dsets\n+                    if self.__profile_reg_check.match(n)\n+                ]\n+                if not profile_dset_names:\n+                    msg = (\"Did not find any data sets matching the regex: \"\n+                           \"{!r} in {!r}. Please ensure that the profile data \"\n+                           \"exists and that the data set is named correctly.\")\n+                    e = msg.format(PROFILE_DSET_REGEX, fp)\n+                    logger.error(e)\n+                    raise FileInputError(e)\n+                elif len(profile_dset_names) > 1:\n+                    msg = (\"Found more than one profile in {!r}: {}. \"\n+                           \"This module is not intended for hybridization of \"\n+                           \"multiple representative profiles. Please re-run \"\n+                           \"on a single aggregated profile.\")\n+                    e = msg.format(fp, profile_dset_names)\n+                    logger.error(e)\n+                    raise FileInputError(e)\n+                else:\n+                    self.profile_dset_names += profile_dset_names\n+\n+    def _validate_merge_col_exists(self):\n+        \"\"\"Validate the existence of the merge column.\n+\n+        Raises\n+        ------\n+        FileInputError\n+            If merge column is missing from either the solar or\n+            the wind meta data.\n+        \"\"\"\n+        msg = (\"Cannot hybridize: merge column {!r} missing from the \"\n+               \"{} meta data! ({!r})\")\n+\n+        mc = ColNameFormatter.fmt(MERGE_COLUMN)\n+        for cols, fp, res in zip([self.__solar_cols, self.__wind_cols],\n+                                 [self.solar_fpath, self.wind_fpath],\n+                                 ['solar', 'wind']):\n+            if mc not in cols:\n+                e = msg.format(MERGE_COLUMN, res, fp)\n+                logger.error(e)\n+                raise FileInputError(e)\n+\n+    def _validate_unique_merge_col(self):\n+        \"\"\"Validate the existence of unique values in the merge column.\n+\n+        Raises\n+        ------\n+        FileInputError\n+            If merge column contains duplicate values  in either the solar or\n+            the wind meta data.\n+        \"\"\"\n+        msg = (\"Duplicate {}s were found. This is likely due to resource \"\n+               \"class binning, which is not supported at this time. \"\n+               \"Please re-run supply curve aggregation without \"\n+               \"resource class binning and ensure there are no duplicate \"\n+               \"values in {!r}. File: {!r}\")\n+\n+        mc = ColNameFormatter.fmt(MERGE_COLUMN)\n+        for ds, cols, fp in zip([self.solar_meta, self.wind_meta],\n+                                [self.__solar_cols, self.__wind_cols],\n+                                [self.solar_fpath, self.wind_fpath]):\n+            merge_col = ds.columns[cols == mc].item()\n+            if not ds[merge_col].is_unique:\n+                e = msg.format(merge_col, merge_col, fp)\n+                logger.error(e)\n+                raise FileInputError(e)\n+\n+    def _validate_merge_col_overlaps(self):\n+        \"\"\"Validate the existence of overlap in the merge column values.\n+\n+        Raises\n+        ------\n+        FileInputError\n+            If merge column values do not overlap between the tow input files.\n+        \"\"\"\n+        mc = ColNameFormatter.fmt(MERGE_COLUMN)\n+        merge_col = self.solar_meta.columns[self.__solar_cols == mc].item()\n+        solar_vals = set(self.solar_meta[merge_col].values)\n+        merge_col = self.wind_meta.columns[self.__wind_cols == mc].item()\n+        wind_vals = set(self.wind_meta[merge_col].values)\n+        self.merge_col_overlap_values = solar_vals & wind_vals\n+\n+        if not self.merge_col_overlap_values:\n+            msg = (\"No overlap detected in the values of {!r} across the \"\n+                   \"input files. Please ensure that at least one of the \"\n+                   \"{!r} values is the same for input files {!r} and {!r}\")\n+            e = msg.format(merge_col, merge_col, self.solar_fpath,\n+                           self.wind_fpath)\n+            logger.error(e)\n+            raise FileInputError(e)\n+\n+\n+class MetaHybridizer:\n+    \"\"\"Framework to handle hybridization of meta data.\"\"\"\n+\n+    _INTERNAL_COL_PREFIX = '_h_internal'\n+\n+    def __init__(self, data, allow_solar_only=False,\n+                 allow_wind_only=False, fillna=None,\n+                 limits=None, ratios=None):\n+        \"\"\"\n+        Parameters\n+        ----------\n+        data : `HybridsData`\n+            Instance of `HybridsData` containing input data to hybridize.\n+        allow_solar_only : bool, optional\n+            Option to allow SC points with only solar capcity (no wind), by\n+            default False.\n+        allow_wind_only : bool, optional\n+            Option to allow SC points with only wind capcity (no solar), by\n+            default False.\n+        fillna : dict, optional\n+            Dictionary containing column_name, fill_value pairs reprenting any\n+            fill values that should be applied after merging the wind and solar\n+            meta. Note that column names will likely have to be prefixed\n+            with \"solar_\" or \"wind_\". By default None.",
    "author_association": "COLLABORATOR",
    "commit_id": "baa786f777fa41fbe3281fd36b9682d03ab16e76",
    "id": 817010668,
    "repo": "NREL/reV",
    "extension": ".py"
  },
  {
    "html_url": "https://github.com/mila-iqia/blocks/pull/10#discussion_r19237183",
    "path": "blocks/bricks.py",
    "line": 839.0,
    "body": "Probably yes.. I should say that I don't really understand what they \nare. I just know that they are usually None for the forward-pass.. :)\n\nI will add it to todo in the docstring.\n\nOn 22.10.2014 19:40, Bart van Merri\u00ebnboer wrote:\n\n> In blocks/bricks.py:\n> \n> > +\n> > -                def scan_function(*args):\n> > -                    args = list(args)\n> > -                    arg_names = (inputs_given.keys()\n> > -                            + states_given.keys()\n> > -                            + contexts_given.keys())\n> > -                    kwargs = dict(zip(arg_names, args))\n> > -                    kwargs.update(rest_kwargs)\n> > -                    return fun(self, **kwargs)\n> > -                result, updates = theano.scan(scan_function,\n> > -                        sequences=inputs_given.values(),\n> > -                        outputs_info=states_given.values()\n> > -                                + [None] \\* num_outputs,\n> > -                        non_sequences=contexts_given.values(),\n> > -                        go_backwards=reverse)\n> > -                assert not updates\n> \n> I guess that eventually we'll need to come up with a way to actually \n> store these updates somewhere where they can be collected before \n> compiling the function.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub \n> https://github.com/bartvm/blocks/pull/10/files#r19230352.\n",
    "user": "rizar",
    "diff_hunk": "@@ -702,7 +703,142 @@ def apply(self, inp):\n         return output.reshape(full_shape)\n \n \n-class Recurrent(DefaultRNG):\n+class BaseRecurrent(Brick):\n+    \"\"\"Base class for recurrent bricks.\n+\n+    A recurrent network processes sequences by applying recursively\n+    a transition operator. This class contains some useful routine\n+    that fascilitate simple and boilerplate code free implementation\n+    of recurrent bricks.\n+    \"\"\"\n+\n+    def zero_state(self, batch_size):\n+        \"\"\"Create an initial state consisting of zeros.\n+\n+        The default state initialization routine. The dtype of the\n+        state is extracted from `self.params`. If there are parameters\n+        with different dtypes, a smarter method should be used.\n+        \"\"\"\n+        dtype = self.params[0].dtype\n+        for i in self.params[1:]:\n+            assert self.params[i].dtype == dtype\n+        return tensor.zeros((batch_size, self.dim), dtype=dtype)\n+\n+    @staticmethod\n+    def recurrent_apply_method(inputs, states, contexts=[], num_outputs=0):\n+        \"\"\"Wraps an apply method to allow its recurrent application.\n+\n+        This decorator allows you to use implementation\n+        of an RNN transition to process sequences without writing\n+        the iteration-related code again and again. In the most general\n+        form information flow of a recurrent network\n+        can be described as follows: depending on the context variables\n+        and driven by input sequences the RNN updates its states and\n+        produces output sequences. Thus the input variables of\n+        your transition function play one of three roles: an input,\n+        a context or a state. These roles should be specified it the\n+        decorator call to make iteration possible.\n+\n+        Parameters\n+        ----------\n+        contexts : list of strs\n+            Names of transition function arguments that\n+            play context role.\n+        inputs : list of strs\n+            Names of transition function argument that\n+            play input role.\n+        states : list of str or (str, function) tuples.\n+            Names of transition function arguments that\n+            play state role. Additionaly a state initialization\n+            function can be passed. The function should take\n+            `self` and the batch sizes as arguments. By\n+            default `BaseRecurrent.zero_state` is used.\n+        num_outputs : int\n+            Number of outputs of the transition function.\n+        \"\"\"\n+\n+        # Take care of default initialization.\n+        for i in range(len(states)):\n+            if isinstance(states[i], str):\n+                states[i] = (states[i], BaseRecurrent.zero_state)\n+\n+        states, state_init_funcs = map(list, zip(*states))\n+        scan_names = contexts + inputs + states\n+\n+        def decorator(fun):\n+            arg_spec = inspect.getargspec(fun)\n+            arg_names = arg_spec.args[1:]\n+\n+            def actual_apply(self, *args, **kwargs):\n+                \"\"\"Iterates a transition function.\n+\n+                Parameters\n+                ----------\n+                one_step : bool\n+                    If True no iteration is done, transition function is simply\n+                    applied to the arguments. If not given is False.\n+                reverse : bool\n+                    If True, the inputs are processed in backward direction.\n+                    It not given is False.\n+                \"\"\"\n+\n+                # Extract arguments related to iteration.\n+                one_step = kwargs.pop(\"one_step\", False)\n+                if one_step:\n+                    return fun(self, *args, **kwargs)\n+                reverse = kwargs.pop(\"reverse\", False)\n+                assert not reverse or not one_step\n+\n+                # Push everything to kwargs.\n+                for arg, arg_name in zip(args, arg_names):\n+                    kwargs[arg_name] = arg\n+                # Separate kwargs that are not\n+                # input, context or state variables.\n+                rest_kwargs = {key : value for key, value in kwargs.items()\n+                        if not key in scan_names}\n+\n+                # Check what is given and what is not.\n+                def only_given(arg_names):\n+                    return OrderedDict((arg_name, kwargs[arg_name])\n+                            for arg_name in arg_names\n+                            if arg_name in kwargs)\n+                inputs_given = only_given(inputs)\n+                contexts_given = only_given(contexts)\n+\n+                # At least one input, please.\n+                assert len(inputs_given) > 0\n+                some_input = inputs_given.values()[1]\n+                batch_size = some_input.shape[1]\n+\n+                # Ensure that all initial states are available.\n+                for state, init_func in zip(states, state_init_funcs):\n+                    if not kwargs.get(state):\n+                        kwargs[state] = init_func(self, batch_size)\n+                states_given = only_given(states)\n+                assert len(states_given) == len(states)\n+\n+                def scan_function(*args):\n+                    args = list(args)\n+                    arg_names = (inputs_given.keys()\n+                            + states_given.keys()\n+                            + contexts_given.keys())\n+                    kwargs = dict(zip(arg_names, args))\n+                    kwargs.update(rest_kwargs)\n+                    return fun(self, **kwargs)\n+                result, updates = theano.scan(scan_function,\n+                        sequences=inputs_given.values(),\n+                        outputs_info=states_given.values()\n+                                + [None] * num_outputs,\n+                        non_sequences=contexts_given.values(),\n+                        go_backwards=reverse)\n+                assert not updates",
    "author_association": "CONTRIBUTOR",
    "commit_id": "3f419466fcfb406511044c26ecd9650e90ffa901",
    "id": 19237183,
    "repo": "mila-iqia/blocks",
    "extension": ".py"
  },
  {
    "html_url": "https://github.com/analysiscenter/batchflow/pull/324#discussion_r296187226",
    "path": "batchflow/models/tf/base.py",
    "line": 357.0,
    "body": "Update docstring about session config",
    "user": "roman-kh",
    "diff_hunk": "@@ -242,76 +244,150 @@ def __init__(self, *args, **kwargs):\n         self.session = kwargs.get('session', None)\n         self.graph = tf.Graph() if self.session is None else self.session.graph\n         self._graph_context = None\n-        self.is_training = None\n-        self.global_step = None\n-        self.loss = None\n-        self.microbatch = None\n+        self._full_config = {}\n+\n+        # Train configurations\n         self.train_steps = None\n-        self.optimizers = {}\n         self._train_lock = threading.Lock()\n-        self._full_config = {}\n-        self._attrs = []\n+\n+        # Parameters of batch processing: splitting batches into parts and/or using multiple devices to process data\n+        self.microbatch = None\n+        self.devices = []\n+        self.leading_device = None\n+        self.device_to_scope = {}\n+        self.multi_device = False\n+\n+        # Easy to access tensors\n+        self._attrs = {}\n         self._saver = None\n         self._to_classes = {}\n         self._inputs = {}\n-        self.inputs = None\n \n         super().__init__(*args, **kwargs)\n \n-    def build(self, *args, **kwargs):\n-        \"\"\" Build the model\n-\n-        #. Create a graph\n-        #. Define ``is_training`` and ``global_step`` tensors\n-        #. Define a model architecture by calling :meth:``._build``\n-        #. Create a loss function from config\n-        #. Create an optimizer and define a train step from config\n-        #. `Set UPDATE_OPS control dependency on train step\n-           <https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization>`_\n-        #. Create a tensorflow session\n-        \"\"\"\n-\n-        def _device_context():\n-            if 'device' in self.config:\n-                device = self.config.get('device')\n-                context = self.graph.device(device)\n+    def store_to_attr(self, attr, graph_item, device=None):\n+        \"\"\" Store `graph_item` to private container.\"\"\"\n+        if self._attrs.get(attr) is None:\n+            if device is None:\n+                self._attrs[attr] = graph_item\n+            else:\n+                self._attrs[attr] = {device: graph_item}\n+        else:\n+            if device is None:\n+                print('WARNING: attr is {}, device is {}'.format(attr, device))\n+                self._attrs[attr] = graph_item\n+            else:\n+                self._attrs[attr][device] = graph_item\n+\n+    def get_from_attr(self, attr, device=None):\n+        \"\"\" Get item from private container or directly from model graph.\"\"\"\n+        if attr in self._attrs:\n+            if isinstance(self._attrs[attr], dict):\n+                device = device or self.leading_device\n+                if device in self._attrs[attr]:\n+                    graph_item = self._attrs[attr][device]\n+                else:\n+                    graph_item = self._attrs[attr]\n             else:\n-                context = contextlib.ExitStack()\n-            return context\n+                graph_item = self._attrs[attr]\n+        else:\n+            graph_item = self._check_tensor(attr, device)\n+        return graph_item\n+\n+    def _check_tensor(self, name, device=None):\n+        prefix = self.__class__.__name__ + '/'\n+        if device is not None:\n+            if device in self.device_to_scope.keys():\n+                prefix += self.device_to_scope[device]\n+            else:\n+                prefix += device\n \n-        with self.graph.as_default(), _device_context():\n+        valid = [item for item in self.graph.get_operations()\n+                 if (name in item.name) and (item.name.startswith(prefix))]\n+        if len(valid) != 1:\n+            valid = [item for item in valid if item.name.endswith('_output')]\n+\n+        if len(valid) == 1:\n+            return valid[0].values()[0]\n+\n+        if len(valid) > 1:\n+            raise KeyError(\"Too many tensors match the '%s' name in  %s model\" % (name, type(self).__name__))\n+        raise KeyError(\"Model %s does not have '%s' tensor\" % (type(self).__name__, name))\n+\n+    def build(self, *args, **kwargs):\n+        \"\"\" Build the model. \"\"\"\n+        # Get list of all available devices, infer leading device and number of devices\n+        self.devices = self._get_devices()\n+        if len(self.devices) > 1:\n+            self.multi_device = len(self.devices)\n+        self.leading_device = self.devices[0]\n+\n+        self.device_to_scope = {item: item[1:].replace(':', '_') for item in self.devices}\n+        self.scope_to_device = {v: k for k, v in self.device_to_scope.items()}\n+\n+        # Create model graph. First of all, `is_training` and `global_step` tensors are defined;\n+        # then, for each device, model architecture is created (with inputs placeholders and all);\n+        # finally, individual train steps with desired loss, optimizer, decay and scope are created\n+        with self.graph.as_default():\n             with tf.variable_scope(self.__class__.__name__):\n                 with tf.variable_scope('globals'):\n-                    if self.is_training is None:\n-                        self.store_to_attr('is_training', tf.placeholder(tf.bool, name='is_training'))\n-                    if self.global_step is None:\n-                        self.store_to_attr('global_step', tf.Variable(0, trainable=False, name='global_step'))\n-\n-                config = self.build_config()\n-                self._full_config = config\n-                self._build(config)\n+                    is_training = tf.placeholder(tf.bool, name='is_training')\n+                    self.store_to_attr('is_training', is_training)\n+\n+                    global_step = tf.Variable(0, trainable=False, name='global_step')\n+                    self.store_to_attr('global_step', global_step)\n+\n+                for device in self.devices:\n+                    with tf.device(device):\n+                        with tf.variable_scope(self.device_to_scope[device]):\n+                            self._inputs = {}\n+                            config = self.build_config()\n+                            self._full_config = config\n+                            self._build(config)\n+\n                 self.microbatch = config.get('microbatch')\n \n                 if self.session is None:\n                     self.create_session(config)\n-                    self.reset()\n \n-                if self.train_steps is None:\n-                    self._make_train_steps(config)\n-                else:\n-                    self.store_to_attr('train_steps', self.train_steps)\n+                self._make_train_steps(config)\n+                self.reset()\n \n     def create_session(self, config=None):\n         \"\"\" Create TF session \"\"\"\n         config = config if config is not None else self.config\n         session_config = config.get('session', default={})\n-        self.session = tf.Session(**session_config)\n+        session_config = {**session_config, **{'allow_soft_placement': True}}",
    "author_association": "MEMBER",
    "commit_id": "ec11119b988274ebc04c005227d64b0959cfb688",
    "id": 296187226,
    "repo": "analysiscenter/batchflow",
    "extension": ".py"
  },
  {
    "html_url": "https://github.com/astropy/astropy/pull/3067#discussion_r19736856",
    "path": "astropy/io/fits/hdu/hdulist.py",
    "line": 186.0,
    "body": "Just update this docstring and this should be fine.  Something more along the lines of what it actually tests.  For example\n\n```\n\"\"\"\nReturns `True` if ``HDUList.index_of(item)`` succeeds.\n\"\"\"\n```\n",
    "user": "embray",
    "diff_hunk": "@@ -180,6 +180,16 @@ def __getitem__(self, key):\n         idx = self.index_of(key)\n         return super(HDUList, self).__getitem__(idx)\n \n+    def __contains__(self, item):\n+        \"\"\"\n+        Used by the 'in' operator\n+        \"\"\"",
    "author_association": "MEMBER",
    "commit_id": "a02bdb2fcfe073e9dd104b5977aface14d1b5491",
    "id": 19736856,
    "repo": "astropy/astropy",
    "extension": ".py"
  },
  {
    "html_url": "https://github.com/pretix/pretix/pull/334#discussion_r89618197",
    "path": "src/pretix/base/payment.py",
    "line": 360.0,
    "body": "Please update the docstring here, aswell.",
    "user": "rixx",
    "diff_hunk": "@@ -339,7 +354,7 @@ def order_change_allowed(self, order: Order) -> bool:\n \n         :param order: The order object\n         \"\"\"\n-        return True\n+        return self._is_still_available()",
    "author_association": "MEMBER",
    "commit_id": "2f04d4272118bbcc2fd5308998d60d83ae4add94",
    "id": 89618197,
    "repo": "pretix/pretix",
    "extension": ".py"
  },
  {
    "html_url": "https://github.com/boto/boto/pull/3174#discussion_r30731087",
    "path": "boto/route53/connection.py",
    "line": 156.0,
    "body": "Cool - updated, as well as updated the get_all_hosted_zones method, as that's what I based my docstring off of.\n",
    "user": "phobologic",
    "diff_hunk": "@@ -136,6 +136,48 @@ def get_all_hosted_zones(self, start_marker=None, zone_list=None):\n             e = self.get_all_hosted_zones(next_marker, zone_list)\n         return e\n \n+    def get_all_hosted_zones_by_name(self, dns_name=None, hosted_zone_id=None,\n+                                     zone_list=None):\n+        \"\"\"\n+        Returns a Python data structure with information about all\n+        Hosted Zones defined for the AWS account.\n+\n+        :param string dns_name: Specifies the first dnsname to be returned in\n+            the list of domains. Mostly meant to be used with hosted_zone_id.\n+            If left blank, will return all dns records.\n+        :param string hosted_zone_id: Should not be included in first query\n+            to get_all_hosted_zones_by_name, but should be used, along with\n+            dns_name when dealing with truncated lists.\n+        :param list zone_list: a HostedZones list to prepend to results",
    "author_association": "CONTRIBUTOR",
    "commit_id": "e72adb934638b9d917098bd3c79db7b676371527",
    "id": 30731087,
    "repo": "boto/boto",
    "extension": ".py"
  },
  {
    "html_url": "https://github.com/Project-MONAI/MONAI/pull/384#discussion_r424687277",
    "path": "monai/data/dataset.py",
    "line": 48.0,
    "body": "this is nice simplification, need to update the docstring, and i'm surprised that there's no need to change any unit tests for this...",
    "user": "wyli",
    "diff_hunk": "@@ -45,10 +45,7 @@ def __init__(self, data, transform=None):\n             transform (Callable, optional): transforms to execute operations on input data.\n         \"\"\"\n         self.data = data\n-        if isinstance(transform, Compose):\n-            self.transform = transform\n-        else:\n-            self.transform = Compose(ensure_tuple(transform))\n+        self.transform = transform",
    "author_association": "MEMBER",
    "commit_id": "b294657482d370242c5375f32ecab4b6def470b1",
    "id": 424687277,
    "repo": "Project-MONAI/MONAI",
    "extension": ".py"
  },
  {
    "html_url": "https://github.com/matplotlib/matplotlib/pull/5942#discussion_r51233494",
    "path": "lib/matplotlib/backends/backend_agg.py",
    "line": 323.0,
    "body": "Should the docstring be updated with this change\n",
    "user": "jenshnielsen",
    "diff_hunk": "@@ -321,7 +321,7 @@ def option_scale_image(self):\n         \"\"\"\n         agg backend support arbitrary scaling of image.\n         \"\"\"",
    "author_association": "MEMBER",
    "commit_id": "bda45b683c5f300f430041eb09b4f9d4d0808a51",
    "id": 51233494,
    "repo": "matplotlib/matplotlib",
    "extension": ".py"
  },
  {
    "html_url": "https://github.com/forseti-security/forseti-security/pull/2616#discussion_r262571487",
    "path": "google/cloud/forseti/common/gcp_type/groups_settings.py",
    "line": 15.0,
    "body": "Nit: update docstring.",
    "user": "ahoying",
    "diff_hunk": "@@ -0,0 +1,76 @@\n+# Copyright 2017 The Forseti Security Authors. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"A CryptoKey object.",
    "author_association": "COLLABORATOR",
    "commit_id": "37c3d042bd8945bd4b79e5ed42da70e1a321c28f",
    "id": 262571487,
    "repo": "forseti-security/forseti-security",
    "extension": ".py"
  },
  {
    "html_url": "https://github.com/aws/deep-learning-containers/pull/2078#discussion_r917188297",
    "path": "test/test_utils/__init__.py",
    "line": 1523.0,
    "body": "In what cases would you provide an empty framework version? If it should truly be optional, can you update the docstring to include that it is optional, and can be '' - alternatively you could just update the default value in the arg to `framework_version=''`",
    "user": "arjkesh",
    "diff_hunk": "@@ -1510,6 +1510,35 @@ def get_python_version_from_image_uri(image_uri):\n     python_version = python_version_search.group()\n     return \"py36\" if python_version == \"py3\" else python_version\n \n+def construct_buildspec_path(dlc_path, framework_path, buildspec, framework_version):\n+    \"\"\"\n+    Construct a relative path to the buildspec yaml file by iterative checking on the existence of\n+    a specific version file for the framework being tested. Possible options include:\n+    [buildspec-[Major]-[Minor]-[Patch].yml, buildspec-[Major]-[Minor].yml, buildspec-[Major].yml, buildspec.yml]\n+    :param dlc_path: path to the DLC test folder\n+    :param framework_path: Framework folder name\n+    :param buildspec: buildspec file name\n+    :param framework_version: default (long) framework version name\n+    \"\"\"\n+    if framework_version:",
    "author_association": "CONTRIBUTOR",
    "commit_id": "71b27baf49731d7e1f7ff1c764de654d62e08693",
    "id": 917188297,
    "repo": "aws/deep-learning-containers",
    "extension": ".py"
  }
]