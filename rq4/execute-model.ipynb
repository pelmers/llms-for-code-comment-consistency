{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook loads a model output from train-models and executes it on a real repository\n",
    "\n",
    "# A list of repo names\n",
    "REPO_NAMES = [\"apache/tomcat\", \"dbeaver/dbeaver\", \"hortonworks/cloudbreak\",\n",
    "              \"kiegroup/kie-tools\", \"apereo/cas\", \"jenkinsci/jenkins\",\n",
    "              \"bazelbuild/bazel\", \"quarkusio/quarkus\", \"apache/camel\", \"vespa-engine/vespa\",\n",
    "              \"bakdata/conquery\", \"project-ncl/bacon\"]\n",
    "FILE_EXTENSION = \".java\"\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "MASTER_PROJECT_REPO_URL = 'https://github.com/pelmers/llms-for-code-comment-consistency.git'\n",
    "\n",
    "USE_WANDB = True\n",
    "WANDB_KEY = '1a0427f55873ebb00be03054c1dc8e4fee78a637'\n",
    "\n",
    "# Read from the wandb run page, e.g. codegen_sz350M_bs8_lr1e-5_epochs10_len128\n",
    "# WANDB_RUN_NAME = \"\"\n",
    "RUN_NAME = \"codebert_szbase_bs6_lr1e-05_len512_langpy_tune_extras\"\n",
    "PREC_OR_F1 = \"f1\"\n",
    "# None to load from file, run path format: <username>/<project-name>/<run-id>\n",
    "# WANDB_RESTORE_RUN_PATH = \"pelmers/codegen-model-master/4qrh993h\"\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# If running as a script, allow os.environ to overwrite these options\n",
    "if __name__ == '__main__':\n",
    "    import os\n",
    "    for k, v in os.environ.items():\n",
    "        if k in globals():\n",
    "            # First check if v is a boolean or a number and convert to the right type\n",
    "            if v.lower() == 'true':\n",
    "                v = True\n",
    "            elif v.lower() == 'false':\n",
    "                v = False\n",
    "            elif v.isnumeric():\n",
    "                v = int(v)\n",
    "            # Or a float\n",
    "            elif '.' in v and v.replace('.', '').isnumeric():\n",
    "                v = float(v)\n",
    "            # Or a list\n",
    "            elif v.startswith('[') and v.endswith(']'):\n",
    "                v = v[1:-1].split(',')\n",
    "                v = [a.strip() for a in v]\n",
    "            globals()[k] = v\n",
    "\n",
    "if DEBUG:\n",
    "    USE_WANDB = False\n",
    "\n",
    "CHECKPOINT_FILE = f\"checkpoints/ckpt_{RUN_NAME}_best_{PREC_OR_F1}.pt\"\n",
    "\n",
    "import sys, subprocess, os\n",
    "\n",
    "# Define function x that given a command string, runs it with subprocess and streams the output\n",
    "def x(cmd):\n",
    "    return subprocess.run(cmd.split(\" \")).returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If ubuntu then install build-essential\n",
    "if x('uname') == 0:\n",
    "    x('sudo apt-get install -y build-essential')\n",
    "\n",
    "# Install pip packages\n",
    "assert x('pip install -U tree_sitter pydriller wandb transformers tokenizers scikit-learn matplotlib') == 0\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "    assert x('pip install -U matplotlib==3.1.3') == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone repo if this is just the notebook file (current folder is not 'rq1'), then cd to the cloned repo\n",
    "if not os.path.abspath(os.getcwd()).endswith('rq4'):\n",
    "    # If llms-for-code-comment-consistency exists, then go in and pull any updates\n",
    "    if os.path.exists('llms-for-code-comment-consistency'):\n",
    "        os.chdir('llms-for-code-comment-consistency')\n",
    "        # Update if already exists\n",
    "        try:\n",
    "            assert x('git pull origin main --ff-only') == 0\n",
    "        except AssertionError:\n",
    "            # old version of git doesn't support --ff-only\n",
    "            assert x('git pull origin main') == 0\n",
    "    else:\n",
    "        assert x(f'git clone {MASTER_PROJECT_REPO_URL}') == 0\n",
    "        os.chdir('llms-for-code-comment-consistency')\n",
    "else:\n",
    "    # cd to parent of this folder for the root of the repo\n",
    "    os.chdir(os.path.dirname(os.getcwd()))\n",
    "\n",
    "sys.path.append('lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.login(key=WANDB_KEY)\n",
    "    run = wandb.init(project=f\"execute-model-master\")\n",
    "else:\n",
    "    wandb.init(project=f\"execute-model-master\", mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Model loading\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m get_model\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLoading previous checkpoint...\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/School/master-project-git/notebooks/lib/models.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_model\u001b[39m(model_type, model_size):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/master-project/lib/python3.9/site-packages/torch/__init__.py:650\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[39m################################################################################\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[39m# Define Storage and Tensor classes\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[39m################################################################################\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_tensor\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[0;32m--> 650\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mstorage\u001b[39;00m \u001b[39mimport\u001b[39;00m _StorageBase, TypedStorage, _LegacyStorage, UntypedStorage\n\u001b[1;32m    652\u001b[0m \u001b[39m# NOTE: New <type>Storage classes should never be added. When adding a new\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \u001b[39m# dtype, use torch.storage.TypedStorage directly.\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mByteStorage\u001b[39;00m(_LegacyStorage):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/master-project/lib/python3.9/site-packages/torch/storage.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m lru_cache\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     HAS_NUMPY \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/master-project/lib/python3.9/site-packages/numpy/__init__.py:151\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m fft\n\u001b[1;32m    150\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m polynomial\n\u001b[0;32m--> 151\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m random\n\u001b[1;32m    152\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m ctypeslib\n\u001b[1;32m    153\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m ma\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/master-project/lib/python3.9/site-packages/numpy/random/__init__.py:180\u001b[0m\n\u001b[1;32m    126\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m    127\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mbeta\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    128\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mbinomial\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mzipf\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    177\u001b[0m ]\n\u001b[1;32m    179\u001b[0m \u001b[39m# add these for module-freeze analysis (like PyInstaller)\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _pickle\n\u001b[1;32m    181\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _common\n\u001b[1;32m    182\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _bounded_integers\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/master-project/lib/python3.9/site-packages/numpy/random/_pickle.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmtrand\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomState\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_philox\u001b[39;00m \u001b[39mimport\u001b[39;00m Philox\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_pcg64\u001b[39;00m \u001b[39mimport\u001b[39;00m PCG64, PCG64DXSM\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:398\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model loading\n",
    "from models import get_model\n",
    "\n",
    "import torch\n",
    "\n",
    "print('Loading previous checkpoint...')\n",
    "\n",
    "print(f'Loading from local file {CHECKPOINT_FILE}...')\n",
    "prev_state = torch.load(CHECKPOINT_FILE)\n",
    "\n",
    "model_type = prev_state['model_type']\n",
    "model_size = prev_state['model_size']\n",
    "max_length = prev_state['max_length']\n",
    "\n",
    "print('Preparing model state...')\n",
    "my_model, tokenizer, config = get_model(model_type, model_size)\n",
    "\n",
    "my_model.load_state_dict(prev_state['model_state_dict'])\n",
    "start_epoch = prev_state['epoch']\n",
    "positive_threhshold = prev_state['positive_threshold']\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    my_model.cuda()\n",
    "    x('nvidia-smi')\n",
    "\n",
    "print(f'Loaded model {model_type} {model_size} from epoch {start_epoch}, file {CHECKPOINT_FILE}')\n",
    "\n",
    "if USE_WANDB:\n",
    "    run.name = f'execute-{len(REPO_NAMES)}-{FILE_EXTENSION}-repos-model-{model_type}-{model_size}-{max_length}'\n",
    "    run.notes = f'''\n",
    "    Model path: {CHECKPOINT_FILE}\n",
    "    Repo list: {str(REPO_NAMES)}\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_repo(repo_name, repo_folder):\n",
    "    assert(x(f'mkdir -p {repo_folder}') == 0)\n",
    "    assert(x(f'rm -rf {repo_folder}') == 0)\n",
    "    repo_url = f'https://github.com/{repo_name}.git'\n",
    "    assert(x(f'git clone {repo_url} {repo_folder} --depth=1') == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "from parsers import parse_entire_directory, create_dataset_example_object\n",
    "from data import dataset_from_file, get_collate_fn\n",
    "\n",
    "collate_fn = get_collate_fn(tokenizer)\n",
    "\n",
    "def parse_repo_to_dataset(repo_name, repo_folder):\n",
    "    print(f'Parsing all {FILE_EXTENSION} files in {repo_folder}...')\n",
    "    repo_examples = parse_entire_directory(repo_folder, FILE_EXTENSION)\n",
    "    # Add a 'github_url' field to each example\n",
    "    for ex in repo_examples:\n",
    "        ex['github_url'] = f'https://github.com/{repo_name}/blob/master/{ex[\"path\"]}#L{ex[\"comment_start_line\"] + 1}'\n",
    "    dataset_examples = [create_dataset_example_object({'label': 0}, ex, ex) for ex in repo_examples]\n",
    "    fname = f\"repos/{repo_name.replace('/', '_')}.examples.json\"\n",
    "    with open(fname, 'w') as f:\n",
    "        json.dump(dataset_examples, f, indent=2)\n",
    "\n",
    "    print(f'Examples saved to {fname}, now loading them into a dataset...')\n",
    "\n",
    "    _, dataset = dataset_from_file(fname, model_type, tokenizer, max_length)\n",
    "    return repo_examples, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (620150245.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [1], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    for data_entry in tqdm(dataloader):\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "def execute_model_dataset(dataset):\n",
    "    # Put the dataset in a dataloader and execute the model, final statistics\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    print('Predicting which examples are positive...')\n",
    "\n",
    "    my_model.eval()\n",
    "    predictions = []\n",
    "    probs = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids, attention_mask, labels, _ = batch\n",
    "        if torch.cuda.is_available():\n",
    "            input_ids = input_ids.cuda()\n",
    "            attention_mask = attention_mask.cuda()\n",
    "            labels = labels.cuda()\n",
    "        with torch.no_grad():\n",
    "            prob, loss, _ = my_model(input_ids, attention_mask, labels)\n",
    "            loss += loss.item()\n",
    "            # Predict based on positive_threshold\n",
    "            pred = prob[:, 1] > positive_threhshold\n",
    "            predictions.append(pred.cpu().numpy())\n",
    "            probs.append(prob.cpu().numpy()[:, 1])\n",
    "    predictions = np.concatenate(predictions).tolist()\n",
    "    probs = np.concatenate(probs).tolist()\n",
    "\n",
    "    # Print final statistics\n",
    "    print()\n",
    "    print(f\"Number of examples: {len(predictions)}\")\n",
    "    print(f\"Number of positives: {sum(predictions)}\")\n",
    "    return predictions, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2052471909.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [2], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    print('Printing all positive examples:')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a file, and print all examples\n",
    "\n",
    "def process_results(repo_name, predictions, probs, repo_examples):\n",
    "    results = [{'prediction': prediction, 'prob': prob, 'example': example} for prediction, prob, example in zip(predictions, probs, repo_examples)]\n",
    "    # Thesis: some comments might be copy-pasted, so group those together, maybe one of them is wrong\n",
    "    results_by_comment = {}\n",
    "    for result in results:\n",
    "        comment = result['example']['comment_summary']\n",
    "        if len(comment) == 0:\n",
    "            continue\n",
    "        if comment not in results_by_comment:\n",
    "            results_by_comment[comment] = []\n",
    "        results_by_comment[comment].append(result)\n",
    "    grouped_results = []\n",
    "    ungrouped_results = []\n",
    "    for comment, results in results_by_comment.items():\n",
    "        results = sorted(results, key=lambda x: x['prob'], reverse=True)\n",
    "        name_set = {result['example']['qualified_name'].split('.')[-1] for result in results}\n",
    "        if len(results) > 1 and len(name_set) > 1:\n",
    "            grouped_results.extend(results)\n",
    "        else:\n",
    "            ungrouped_results.extend(results)\n",
    "    positives = [result for result in ungrouped_results if result['prediction'] == True]\n",
    "    negatives = [result for result in ungrouped_results if result['prediction'] == False]\n",
    "    # Sort by seconds_since_last_commit in ascending order, with positives first\n",
    "    results = grouped_results + sorted(positives, key=lambda x: x['example']['seconds_since_last_commit']) + sorted(negatives, key=lambda x: x['example']['seconds_since_last_commit'])\n",
    "\n",
    "    results_name = f'results_{repo_name.replace(\"/\", \"#\")}.json'\n",
    "\n",
    "    import json\n",
    "    with open(results_name, 'w') as f:\n",
    "        json.dump(results, f)\n",
    "        print(f'Saved results to {results_name}...')\n",
    "\n",
    "    if USE_WANDB:\n",
    "        wandb.save(results_name)\n",
    "\n",
    "    print('Printing the most positive examples:')\n",
    "    to_print = 10\n",
    "    from pprint import pprint\n",
    "    for result in sorted(results, key=lambda x: x['prob'], reverse=True)[:to_print]:\n",
    "        pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "x('mkdir -p repos')\n",
    "\n",
    "for repo_name in REPO_NAMES:\n",
    "    print(f'Processing repo {repo_name}...')\n",
    "    # Clone into a temporary folder\n",
    "    with tempfile.TemporaryDirectory() as tmp:\n",
    "        clone_repo(repo_name, tmp)\n",
    "        examples, dataset = parse_repo_to_dataset(repo_name, tmp)\n",
    "        if len(examples) == 0:\n",
    "            print(f'No examples found for {repo_name}, skipping...')\n",
    "            continue\n",
    "        predictions, probs = execute_model_dataset(dataset)\n",
    "        process_results(repo_name, predictions, probs, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a039f0b1bebafebeb17a7df39f3cb9e08c871cbba42b919bd83805750af3b3a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
