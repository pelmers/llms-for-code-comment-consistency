{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09523809523809523\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess, os\n",
    "\n",
    "REPO_URL = 'https://github.com/pelmers/llms-for-code-comment-consistency.git'\n",
    "WANDB_KEY = 'zzz'\n",
    "\n",
    "# Folder of train.json, valid.json, test.json\n",
    "DATA_FOLDER = 'data/deepjit_summary_data'\n",
    "\n",
    "LLM_PROMPT = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
    "LLM_INSTRUCTION = 'Does the following Comment correctly and fully summarize the Code? Respond Yes or No.'\n",
    "LLM_INPUT = 'Comment: {comment}\\nCode: {code}'\n",
    "LLM_OUTPUT_0 = 'Yes.'\n",
    "LLM_OUTPUT_1 = 'No.'\n",
    "\n",
    "# Llama model parameters\n",
    "BASE_MODEL = 'decapoda-research/llama-7b-hf'\n",
    "# other choices: mosaicml/mpt-7b[-instruct], replit/replit-code-v1-3b\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "# In diff mode, the code is given as a diff from the previous version\n",
    "USE_DIFF_MODE = False\n",
    "\n",
    "# Training Parameters\n",
    "BATCH_SIZE = 4\n",
    "MAX_STEPS = 1000\n",
    "EVAL_STEPS = MAX_STEPS // 5\n",
    "WARMUP_STEPS = 50\n",
    "MAX_LEN = 768\n",
    "LEARNING_RATE = 1e-4\n",
    "# Directory to load model, only used when MAX_STEPS is 0\n",
    "LOAD_DIR = None\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "# After training on DATA_FOLDER, also evaluate on ADDITIONAL_TEST_FILES\n",
    "ADDITIONAL_TEST_FILES = []\n",
    "\n",
    "# If running as a script, allow os.environ to overwrite these options\n",
    "if __name__ == '__main__':\n",
    "    import os\n",
    "    for k, v in os.environ.items():\n",
    "        if k in globals():\n",
    "            # First check if v is a boolean or a number and convert to the right type\n",
    "            if v.lower() == 'true':\n",
    "                v = True\n",
    "            elif v.lower() == 'false':\n",
    "                v = False\n",
    "            elif v.isnumeric():\n",
    "                v = int(v)\n",
    "            # Or a float\n",
    "            elif '.' in v and v.replace('.', '').isnumeric():\n",
    "                v = float(v)\n",
    "            # Or a list\n",
    "            elif v.startswith('[') and v.endswith(']'):\n",
    "                v = v[1:-1].split(',')\n",
    "                v = [a.strip() for a in v]\n",
    "            globals()[k] = v\n",
    "\n",
    "# Define function x that given a command string, runs it with subprocess and streams the output\n",
    "def x(cmd):\n",
    "    return subprocess.run(cmd.split(\" \")).returncode\n",
    "\n",
    "RUN_LANGUAGE = 'java'\n",
    "RUN_LANGUAGE = 'go' if 'go' in DATA_FOLDER.lower() else RUN_LANGUAGE\n",
    "RUN_LANGUAGE = 'py' if 'python' in DATA_FOLDER.lower() else RUN_LANGUAGE\n",
    "RUN_LANGUAGE = 'js' if 'javascript' in DATA_FOLDER.lower() or 'js' in DATA_FOLDER.lower() else RUN_LANGUAGE\n",
    "RUN_LANGUAGE = 'all' if 'all' in DATA_FOLDER.lower() else RUN_LANGUAGE\n",
    "\n",
    "RUN_NAME = f\"{BASE_MODEL.split('/')[-1]}\" + \\\n",
    "    f\"_bs{BATCH_SIZE}_len{MAX_LEN}_lang{RUN_LANGUAGE}\" + \\\n",
    "    (\"_diff\" if USE_DIFF_MODE else \"\") + \\\n",
    "    (\"_debug\" if DEBUG else \"\")\n",
    "\n",
    "RUN_NOTES = f\"\"\"Notes:\n",
    "Dataset: {DATA_FOLDER}\n",
    "Max steps: {MAX_STEPS}\n",
    "\n",
    "Learning rate: {LEARNING_RATE}\n",
    "\n",
    "LLM parameters:\n",
    "LORA_R: {LORA_R}\n",
    "LORA_ALPHA: {LORA_ALPHA}\n",
    "LORA_DROPOUT: {LORA_DROPOUT}\n",
    "Prompt: {LLM_PROMPT}\n",
    "Instruction: {LLM_INSTRUCTION}\n",
    "Input: {LLM_INPUT}\n",
    "Output 0: {LLM_OUTPUT_0}\n",
    "Output 1: {LLM_OUTPUT_1}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    os.environ['LD_LIBRARY_PATH'] = '/opt/conda/lib:' + os.environ['LD_LIBRARY_PATH']\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "# create list of 10 0's and 10 1's\n",
    "truths = [0] * 10 + [1] * 10\n",
    "predictions = [1] * 20\n",
    "sample_weights = [19] * 10 + [1] * 10\n",
    "weighted_f1 = sklearn.metrics.f1_score(truths, predictions, sample_weight=sample_weights)\n",
    "print(weighted_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x('pip install -U scikit-learn wandb accelerate transformers[sentencepiece] matplotlib tokenizers bitsandbytes peft datasets') == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# clone repo if this is just the notebook file (current folder is not 'rq1'), then cd to the cloned repo\n",
    "if not os.path.abspath(os.getcwd()).endswith('rq2'):\n",
    "    # If llms-for-code-comment-consistency exists, then go in and pull any updates\n",
    "    if os.path.exists('llms-for-code-comment-consistency'):\n",
    "        os.chdir('llms-for-code-comment-consistency')\n",
    "        # Update if already exists\n",
    "        try:\n",
    "            assert x('git pull origin main --ff-only') == 0\n",
    "        except AssertionError:\n",
    "            # old version of git doesn't support --ff-only\n",
    "            assert x('git pull origin main') == 0\n",
    "    else:\n",
    "        assert x(f'git clone {REPO_URL}') == 0\n",
    "        os.chdir('llms-for-code-comment-consistency')\n",
    "else:\n",
    "    # cd to parent of this folder for the root of the repo\n",
    "    os.chdir(os.path.dirname(os.getcwd()))\n",
    "\n",
    "sys.path.append('lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/ubuntu/miniconda3/lib/python3.10/site-packages (1.2.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (10.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.3/10.3 MB 79.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wandb in /home/ubuntu/miniconda3/lib/python3.10/site-packages (0.15.4)\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/miniconda3/lib/python3.10/site-packages (0.20.3)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /home/ubuntu/miniconda3/lib/python3.10/site-packages (4.30.2)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/miniconda3/lib/python3.10/site-packages (3.7.1)\n",
      "Requirement already satisfied: tokenizers in /home/ubuntu/miniconda3/lib/python3.10/site-packages (0.13.3)\n",
      "Requirement already satisfied: bitsandbytes in /home/ubuntu/miniconda3/lib/python3.10/site-packages (0.39.1)\n",
      "Requirement already satisfied: peft in /home/ubuntu/miniconda3/lib/python3.10/site-packages (0.3.0)\n",
      "Requirement already satisfied: datasets in /home/ubuntu/miniconda3/lib/python3.10/site-packages (2.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from wandb) (1.13.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pathtools in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from wandb) (65.5.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from wandb) (3.1.30)\n",
      "Requirement already satisfied: PyYAML in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: setproctitle in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from accelerate) (22.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.14.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]) (2022.10.31)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.3.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]) (3.9.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.1.97)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from datasets) (2023.5.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from datasets) (1.5.2)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers[sentencepiece]) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ecco 0.1.2 requires scikit-learn~=0.23, but you have scikit-learn 1.3.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed scikit-learn-1.3.0\n"
     ]
    }
   ],
   "source": [
    "def ensure_data_folder(folder):\n",
    "    # If the data folder does not exist, extract it from the .tar.gz file\n",
    "    if not os.path.exists(folder):\n",
    "        print('Extracting data...')\n",
    "        if not os.path.exists(f'{folder}.tar.gz'):\n",
    "            print('Downloading data from server...')\n",
    "            assert x(f'wget -O {folder}.tar.gz https://file2.pelmers.com/{folder}.tar.gz') == 0\n",
    "        assert x(f'tar -xzf {folder}.tar.gz -C data') == 0\n",
    "\n",
    "ensure_data_folder(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpelmers\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/thesis/llms-for-code-comment-consistency/notebooks/wandb/run-20230630_101420-3nzycmth</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pelmers/replit-code-v1-3b-model-master/runs/3nzycmth' target=\"_blank\">genial-eon-9</a></strong> to <a href='https://wandb.ai/pelmers/replit-code-v1-3b-model-master' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pelmers/replit-code-v1-3b-model-master' target=\"_blank\">https://wandb.ai/pelmers/replit-code-v1-3b-model-master</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pelmers/replit-code-v1-3b-model-master/runs/3nzycmth' target=\"_blank\">https://wandb.ai/pelmers/replit-code-v1-3b-model-master/runs/3nzycmth</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=WANDB_KEY)\n",
    "run = wandb.init(project=f\"{BASE_MODEL.split('/')[-1]}-model-master\")\n",
    "if MAX_STEPS == 0:\n",
    "    run.name = RUN_NAME + \"_eval\"\n",
    "else:\n",
    "    run.name = RUN_NAME\n",
    "run.notes = RUN_NOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: CUDA not available, using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)resolve/main/norm.py: 100%|██████████| 2.56k/2.56k [00:00<00:00, 2.75MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/replit/replit-code-v1-3b:\n",
      "- norm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/replit/replit-code-v1-3b:\n",
      "- norm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading (…)n/adapt_tokenizer.py: 100%|██████████| 1.75k/1.75k [00:00<00:00, 3.15MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/replit/replit-code-v1-3b:\n",
      "- adapt_tokenizer.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading (…)in/param_init_fns.py: 100%|██████████| 12.6k/12.6k [00:00<00:00, 19.9MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/replit/replit-code-v1-3b:\n",
      "- param_init_fns.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading (…)solve/main/blocks.py: 100%|██████████| 2.53k/2.53k [00:00<00:00, 2.79MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/replit/replit-code-v1-3b:\n",
      "- blocks.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading (…)meta_init_context.py: 100%|██████████| 3.64k/3.64k [00:00<00:00, 4.01MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/replit/replit-code-v1-3b:\n",
      "- meta_init_context.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading (…)refixlm_converter.py: 100%|██████████| 27.2k/27.2k [00:00<00:00, 41.0MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/replit/replit-code-v1-3b:\n",
      "- hf_prefixlm_converter.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/replit/replit-code-v1-3b:\n",
      "- attention.py\n",
      "- adapt_tokenizer.py\n",
      "- param_init_fns.py\n",
      "- blocks.py\n",
      "- meta_init_context.py\n",
      "- hf_prefixlm_converter.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading pytorch_model.bin: 100%|██████████| 10.4G/10.4G [01:16<00:00, 136MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ubuntu/miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "CUDA SETUP: Loading binary /home/ubuntu/miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "/home/ubuntu/miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "Downloading (…)neration_config.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 46.5kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 400/400 [00:00<00:00, 546kB/s]\n",
      "Downloading (…)plit_lm_tokenizer.py: 100%|██████████| 6.26k/6.26k [00:00<00:00, 4.80MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/replit/replit-code-v1-3b:\n",
      "- replit_lm_tokenizer.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading spiece.model: 100%|██████████| 708k/708k [00:00<00:00, 315MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 87.0/87.0 [00:00<00:00, 90.3kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Adapted hugging face example: https://github.com/huggingface/peft/blob/main/examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py\n",
    "# And: https://github.com/tloen/alpaca-lora/blob/main/finetune.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "dm = 'auto'\n",
    "if DEBUG and not torch.cuda.is_available():\n",
    "    print(\"WARNING: CUDA not available, using CPU\")\n",
    "    dm = 'cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    x('nvidia-smi')\n",
    "\n",
    "if 'llama' in BASE_MODEL:\n",
    "    my_model = LlamaForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=dm,\n",
    "    )\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "else:\n",
    "    # use auto classes\n",
    "    my_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=dm,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    if 'mosaic' in BASE_MODEL:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b', trust_remote_code=True)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in my_model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "try:\n",
    "  my_model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "  my_model.enable_input_require_grads()\n",
    "except:\n",
    "  print(\"WARNING: gradient checkpointing not available\")\n",
    "\n",
    "\n",
    "# Also for stability, cast the output to fp32\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "try:\n",
    "  my_model.lm_head = CastOutputToFloat(my_model.lm_head)\n",
    "except:\n",
    "  print(\"WARNING: could not cast output to float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4194304 || all params: 6742609920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(model)\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "from peft import LoraConfig, get_peft_model \n",
    "\n",
    "target_modules = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "if LOAD_DIR:\n",
    "    # Load peft adapter from checkpoint\n",
    "    from peft import PeftModel\n",
    "    my_model = PeftModel.from_pretrained(my_model, LOAD_DIR)\n",
    "else:\n",
    "    my_model = get_peft_model(my_model, config)\n",
    "\n",
    "print_trainable_parameters(my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/json/default-96de9ed252c3f5d0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|██████████| 1/1 [00:00<00:00, 146.30it/s]\n",
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/json/default-19c4891d74fd8d4b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|██████████| 1/1 [00:00<00:00, 154.89it/s]\n",
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/json/default-ef7a1a55740e6c71/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|██████████| 1/1 [00:00<00:00, 150.51it/s]\n",
      "                                                                \r"
     ]
    }
   ],
   "source": [
    "# Next prepare the dataset\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < MAX_LEN\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    add_eos_token = False\n",
    "    user_prompt = LLM_PROMPT.format(**data_point)\n",
    "    full_prompt = user_prompt + data_point[\"output\"]\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    tokenized_user_prompt = tokenize(\n",
    "        user_prompt, add_eos_token=add_eos_token\n",
    "    )\n",
    "    user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "    if add_eos_token:\n",
    "        user_prompt_len -= 1\n",
    "\n",
    "    tokenized_full_prompt[\"labels\"] = [\n",
    "        -100\n",
    "    ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "        user_prompt_len:\n",
    "    ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "def load_comment_data(json_path):\n",
    "    import datasets, difflib\n",
    "    ds = datasets.load_dataset('json', data_files=json_path)\n",
    "    def create_prompt(x):\n",
    "        code = x['new_code_raw']\n",
    "        if USE_DIFF_MODE:\n",
    "            # Take the diff between old_code_raw and new_code_raw\n",
    "            code = '\\n'.join(difflib.ndiff(x['old_code_raw'].split('\\n'), x['new_code_raw'].split('\\n')))\n",
    "            # remove all lines starting with ? since those are for human readability\n",
    "            code = '\\n'.join([line for line in code.split('\\n') if not line.startswith('?')])\n",
    "        return {'instruction': LLM_INSTRUCTION,\n",
    "                'input': LLM_INPUT.format(comment=x['old_comment_raw'], code=code),\n",
    "                'output': LLM_OUTPUT_0 if x['label'] == 0 else LLM_OUTPUT_1}\n",
    "    # Cast the data to keys 'instruction', 'input', 'output'\n",
    "    ds = ds.shuffle().map(create_prompt).map(generate_and_tokenize_prompt)\n",
    "    # Remove all columns except for the input_ids, attention_mask, and labels\n",
    "    ds = ds.select_columns(['input_ids', 'attention_mask', 'labels'])\n",
    "    return ds['train']\n",
    "\n",
    "train_ds = load_comment_data(os.path.join(DATA_FOLDER, 'train.json'))\n",
    "# TODO: use valid\n",
    "valid_ds = load_comment_data(os.path.join(DATA_FOLDER, 'test.json'))\n",
    "test_ds = load_comment_data(os.path.join(DATA_FOLDER, 'test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Training loop\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\n\u001b[1;32m      5\u001b[0m trainer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m      6\u001b[0m     model\u001b[39m=\u001b[39mmy_model, \n\u001b[1;32m      7\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_ds,\n\u001b[1;32m      8\u001b[0m     eval_dataset\u001b[39m=\u001b[39mvalid_ds,\n\u001b[0;32m----> 9\u001b[0m     args\u001b[39m=\u001b[39mtransformers\u001b[39m.\u001b[39;49mTrainingArguments(\n\u001b[1;32m     10\u001b[0m         per_device_train_batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, \n\u001b[1;32m     11\u001b[0m         gradient_accumulation_steps\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m         warmup_steps\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m         num_train_epochs\u001b[39m=\u001b[39;49mMAX_EPOCHS,\n\u001b[1;32m     14\u001b[0m         learning_rate\u001b[39m=\u001b[39;49m\u001b[39m2e-4\u001b[39;49m, \n\u001b[1;32m     15\u001b[0m         fp16\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     16\u001b[0m         logging_steps\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \n\u001b[1;32m     17\u001b[0m         output_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39moutputs\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     18\u001b[0m     ),\n\u001b[1;32m     19\u001b[0m     data_collator\u001b[39m=\u001b[39mtransformers\u001b[39m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m<string>:111\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/training_args.py:1344\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1337\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1339\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1343\u001b[0m ):\n\u001b[0;32m-> 1344\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1345\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1346\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1347\u001b[0m     )\n\u001b[1;32m   1349\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1350\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1351\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1357\u001b[0m ):\n\u001b[1;32m   1358\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1359\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1360\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--bf16_full_eval`) can only be used on CUDA or CPU/TPU/NeuronCore devices.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1361\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices."
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "import transformers\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=my_model, \n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=BATCH_SIZE, \n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=max(1, 16 // BATCH_SIZE),\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        max_steps=MAX_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=True,\n",
    "        logging_steps=10, \n",
    "        optim=\"adamw_torch\",\n",
    "        report_to=\"wandb\",\n",
    "        output_dir=f'{RUN_NAME}_ckpt',\n",
    "        load_best_model_at_end=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=EVAL_STEPS,\n",
    "        save_steps=EVAL_STEPS,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        eval_accumulation_steps=4,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(tokenizer, return_tensors='pt', padding=True, pad_to_multiple_of=8)\n",
    ")\n",
    "\n",
    "if MAX_STEPS > 0:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Determine whether the following Comment is consistent with the Code.\n",
      "\n",
      "### Input:\n",
      "Comment: Returns the list of operations which are combined in this composite operation.\n",
      "Code: \tpublic List<ITransactionalOperation> getOperations() {\n",
      "\t\treturn operations;\n",
      "\t}\n",
      "\n",
      "\n",
      "\n",
      "### Response:\n",
      "The comment is not consistent.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput: \u001b[39m\u001b[39m{\u001b[39;00mtokenizer\u001b[39m.\u001b[39mdecode(data_point[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGenerated: \u001b[39m\u001b[39m{\u001b[39;00mgenerate(model, tokenizer, data_point)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m print_random_generation(my_model, tokenizer, train_ds)\n",
      "Cell \u001b[0;32mIn[22], line 33\u001b[0m, in \u001b[0;36mprint_random_generation\u001b[0;34m(model, tokenizer, dataset)\u001b[0m\n\u001b[1;32m     31\u001b[0m data_point \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(dataset)\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput: \u001b[39m\u001b[39m{\u001b[39;00mtokenizer\u001b[39m.\u001b[39mdecode(data_point[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGenerated: \u001b[39m\u001b[39m{\u001b[39;00mgenerate(model, tokenizer, data_point)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 20\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, tokenizer, data_point, max_length)\u001b[0m\n\u001b[1;32m     13\u001b[0m generation_config \u001b[39m=\u001b[39m GenerationConfig(\n\u001b[1;32m     14\u001b[0m     temperature\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,\n\u001b[1;32m     15\u001b[0m     top_p\u001b[39m=\u001b[39m\u001b[39m0.75\u001b[39m,\n\u001b[1;32m     16\u001b[0m     top_k\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m,\n\u001b[1;32m     17\u001b[0m     num_beams\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 20\u001b[0m     generation_output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     21\u001b[0m         input_ids\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mtensor(input_ids)\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m),\n\u001b[1;32m     22\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m     23\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     24\u001b[0m         output_scores\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     25\u001b[0m         max_new_tokens\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     27\u001b[0m s \u001b[39m=\u001b[39m generation_output\u001b[39m.\u001b[39msequences[\u001b[39m0\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mdecode(s)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/peft/peft_model.py:731\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[0;32m--> 731\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    732\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1611\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1604\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1605\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1606\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1607\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1608\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1609\u001b[0m     )\n\u001b[1;32m   1610\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1611\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1612\u001b[0m         input_ids,\n\u001b[1;32m   1613\u001b[0m         beam_scorer,\n\u001b[1;32m   1614\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1615\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1616\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1617\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1618\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1619\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1620\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1621\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1622\u001b[0m     )\n\u001b[1;32m   1624\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1625\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1626\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:2909\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2905\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   2907\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2909\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2910\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2911\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2912\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2913\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2914\u001b[0m )\n\u001b[1;32m   2916\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2917\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    687\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    689\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    690\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    691\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    692\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    693\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    694\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    695\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    696\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    697\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    698\u001b[0m )\n\u001b[1;32m    700\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    701\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:578\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    570\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    571\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    572\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    576\u001b[0m     )\n\u001b[1;32m    577\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    579\u001b[0m         hidden_states,\n\u001b[1;32m    580\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    581\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    582\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    583\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    584\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    585\u001b[0m     )\n\u001b[1;32m    587\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    589\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:292\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    289\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    291\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    293\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    294\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    295\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    296\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    297\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    298\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    300\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    302\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:194\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    184\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    185\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     use_cache: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    191\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, Optional[torch\u001b[39m.\u001b[39mTensor], Optional[Tuple[torch\u001b[39m.\u001b[39mTensor]]]:\n\u001b[1;32m    192\u001b[0m     bsz, q_len, _ \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()\n\u001b[0;32m--> 194\u001b[0m     query_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_proj(hidden_states)\u001b[39m.\u001b[39mview(bsz, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    195\u001b[0m     key_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(hidden_states)\u001b[39m.\u001b[39mview(bsz, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    196\u001b[0m     value_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj(hidden_states)\u001b[39m.\u001b[39mview(bsz, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/peft/tuners/lora.py:698\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 698\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(x)\n\u001b[1;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_adapters \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_adapter \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlora_A\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    701\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:402\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m x\u001b[39m.\u001b[39mdtype:\n\u001b[1;32m    400\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 402\u001b[0m out \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mmatmul(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, bias\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate)\n\u001b[1;32m    404\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mhas_fp16_weights:\n\u001b[1;32m    405\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mCB \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mCxB \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m         \u001b[39m# we converted 8-bit row major to turing/ampere format in the first inference pass\u001b[39;00m\n\u001b[1;32m    407\u001b[0m         \u001b[39m# we no longer need the row-major weight\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:562\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[39mif\u001b[39;00m threshold \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m    561\u001b[0m     state\u001b[39m.\u001b[39mthreshold \u001b[39m=\u001b[39m threshold\n\u001b[0;32m--> 562\u001b[0m \u001b[39mreturn\u001b[39;00m MatMul8bitLt\u001b[39m.\u001b[39;49mapply(A, B, out, bias, state)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:296\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    295\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx, A, B, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, bias\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, state\u001b[39m=\u001b[39mMatmulLtState):\n\u001b[0;32m--> 296\u001b[0m     using_igemmlt \u001b[39m=\u001b[39m supports_igemmlt(A\u001b[39m.\u001b[39;49mdevice) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m state\u001b[39m.\u001b[39mforce_no_igemmlt\n\u001b[1;32m    297\u001b[0m     \u001b[39m# default of pytorch behavior if inputs are empty\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     ctx\u001b[39m.\u001b[39mis_empty \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:226\u001b[0m, in \u001b[0;36msupports_igemmlt\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msupports_igemmlt\u001b[39m(device: torch\u001b[39m.\u001b[39mdevice) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m    225\u001b[0m     \u001b[39m\"\"\"check if this device supports the optimized int8 kernel\"\"\"\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mget_device_capability(device\u001b[39m=\u001b[39;49mdevice) \u001b[39m<\u001b[39m (\u001b[39m7\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    227\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     device_name \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mget_device_name(device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:381\u001b[0m, in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_device_capability\u001b[39m(device: Optional[_device_t] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m]:\n\u001b[1;32m    369\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Gets the cuda capability of a device.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \n\u001b[1;32m    371\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m        tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 381\u001b[0m     prop \u001b[39m=\u001b[39m get_device_properties(device)\n\u001b[1;32m    382\u001b[0m     \u001b[39mreturn\u001b[39;00m prop\u001b[39m.\u001b[39mmajor, prop\u001b[39m.\u001b[39mminor\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:395\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_device_properties\u001b[39m(device: _device_t) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    386\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \n\u001b[1;32m    388\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[39m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m     _lazy_init()  \u001b[39m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     device \u001b[39m=\u001b[39m _get_device_index(device, optional\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    397\u001b[0m     \u001b[39mif\u001b[39;00m device \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Print a random generation from the training set\n",
    "\n",
    "my_model.eval()\n",
    "\n",
    "import random\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "max_response_num_words = max(len(LLM_OUTPUT_0.split()), len(LLM_OUTPUT_1.split()))\n",
    "\n",
    "def generate(model, tokenizer, data_point, max_length=max_response_num_words + 1):\n",
    "    input_ids = data_point[\"input_ids\"]\n",
    "    labels = data_point[\"labels\"]\n",
    "    # Remove the user prompt from the input by looking for last label = -100\n",
    "    user_input_ids = input_ids[: len(labels) - list(reversed(labels)).index(-100)]\n",
    "    answer = tokenizer.decode(input_ids[len(user_input_ids):])\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        top_k=40,\n",
    "        num_beams=1,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=torch.tensor(user_input_ids).unsqueeze(0).to(device),\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_length,\n",
    "        )\n",
    "    s = generation_output.sequences[0][len(user_input_ids):]\n",
    "    decoded_s = tokenizer.decode(s)\n",
    "    return decoded_s, answer\n",
    "\n",
    "def print_random_generation(model, tokenizer, dataset):\n",
    "    data_point = random.choice(dataset)\n",
    "    print(f'Datapoint: {tokenizer.decode(data_point[\"input_ids\"], skip_special_tokens=True)}')\n",
    "    output, answer = generate(model, tokenizer, data_point)\n",
    "    print(f'Answer: {answer}')\n",
    "    print(f'Generated: {output}')\n",
    "    label = 1 if LLM_OUTPUT_1 in answer else 0\n",
    "    pred = 1 if LLM_OUTPUT_1 in output else 0\n",
    "    print(f'Correct: {label == pred}')\n",
    "\n",
    "print_random_generation(my_model, tokenizer, train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(model, tokenizer, dataset):\n",
    "    model.eval()\n",
    "    truths = []\n",
    "    predictions = []\n",
    "    bar = tqdm(dataset)\n",
    "    for data_point in bar:\n",
    "        output, answer = generate(model, tokenizer, data_point)\n",
    "        label = 1 if LLM_OUTPUT_1 in answer else 0\n",
    "        pred = 1 if LLM_OUTPUT_1 in output else 0\n",
    "        truths.append(label)\n",
    "        predictions.append(pred)\n",
    "        # Update bar with running accuracy\n",
    "        bar.set_description(f'Accuracy: {sklearn.metrics.accuracy_score(truths, predictions):.3f}')\n",
    "\n",
    "    truths = np.array(truths)\n",
    "    predictions = np.array(predictions)\n",
    "    sample_weights = np.ones(len(truths))\n",
    "    # make the sample_weights array the same size as the number of samples in the dataset, with 1 for positive and weight for negative\n",
    "    sample_weights[truths == 0] = 19\n",
    "    acc = sklearn.metrics.accuracy_score(truths, predictions)\n",
    "    f1 = sklearn.metrics.f1_score(truths, predictions)\n",
    "    weighted_f1 = sklearn.metrics.f1_score(truths, predictions, sample_weight=sample_weights)\n",
    "    precision = sklearn.metrics.precision_score(truths, predictions)\n",
    "    recall = sklearn.metrics.recall_score(truths, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }\n",
    "\n",
    "METRICS_KEYS = ['accuracy', 'f1', 'precision', 'recall', 'weighted_f1']\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Print and log to wandb\n",
    "test_metrics = evaluate(my_model, tokenizer, test_ds)\n",
    "pprint(test_metrics)\n",
    "wandb.log({'test_' + k: test_metrics[k] for k in METRICS_KEYS})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAX_STEPS > 0:\n",
    "    save_name = f'{RUN_NAME}_test_{test_metrics[\"weighted_f1\"]:.3f}'\n",
    "    trainer.save_model(save_name)\n",
    "    print(f'Saved model to {save_name}')\n",
    "    # Tar the folder and upload to wandb\n",
    "    x(f'tar -czf {save_name}.tar.gz {save_name}')\n",
    "    wandb.save(f'{save_name}.tar.gz')\n",
    "    print(f'Uploaded model to wandb run id {wandb.run.id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report results from additional test files\n",
    "for path in ADDITIONAL_TEST_FILES:\n",
    "    folder = os.path.dirname(path)\n",
    "    ensure_data_folder(folder)\n",
    "    print(f'Loaded test set from {path}')\n",
    "    additional_ds = load_comment_data(path)\n",
    "    additional_metrics = evaluate(my_model, tokenizer, additional_ds)\n",
    "    pprint(additional_metrics)\n",
    "    log_prefix = path.replace('.json', '').replace('data/', '')\n",
    "    wandb.log({log_prefix + '_' + k: additional_metrics[k] for k in METRICS_KEYS})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish\n",
    "wandb.finish()\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms-for-code-comment-consistency",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a039f0b1bebafebeb17a7df39f3cb9e08c871cbba42b919bd83805750af3b3a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
